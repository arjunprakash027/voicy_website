we illustrate the concepts needed for the
development of a crawler that collects information from a
dark website. We start from discussing the three layers
of the Internet, the characteristics of the hidden and private
networks, and the technical features of Tor network. We
also addressed the challenges facing the dark web crawler.
Finally, we presented our experimental system that fetches
data from a dark market. This approach helps in putting
a single dark website under investigation, and can be a
seed for future research and development


3.3. Dark Web
It is where most of the illicit activities take place. Such
activities are drugs trading, weapons trading, child abuse,
trading sensitive information, malwares and spywares,
sharing Software Exploits information that hacktivists
discover in computer systems, or renting a Botnet, which
is a full-equipped network connected to the internet that
hackers can operate to perform a wide range security
breach. In addition to trading documents, fake IDs, stolen
credit cards, patients’ medical records, and any other
Personally Identifiable Information (PII). It also includes
financial fraudulence, publicizing criminal ideologies, even
employing hitmen, and a lot more. Dark Web Hidden
Service stake place in the Dark Web as well, they are
special services that host the security breach activities,
and work as the hosting environment for malwares.

We noticed the difference in definitions of Deep Web and
Dark Web among researchers. Some researches define
Deep Web as the part that consists of everything the
search engines cannot discover or index, while Dark Web
consists of hidden networks that use specialized protocols
and software [1] [15]. Other researches define Deep Web
as the web that consists of all of the aforementioned, in
addition to the hidden and private networks that may
contain legal and benign activities, while Dark Web is
that part of the hidden and private networks where illicit
and criminal activities take place

how they work:
1. Decentralization: They mostly use peer-to-peer
techniques where the data stays stored in a group of
personal computers distributed on the network around the
world instead of using a central server.
2. They take advantage of the infrastructure of the public
internet.
3. They use non-standard computer protocols and ports
that make it hard for the users outside the network to
reach.
One of the most famous software that insures these
features is Tor (The Onion Router), in addition to I2P,
Freenet, Hyperboria Network, M-Web, Shadow Web and
others.

. What is a Crawler?
Cheong defines Crawler as “software programs that
traverse the World Wide Web information space by
following hypertext links and retrieving web documents
by standard HTTP protocol”. [17]
Crawlers have many uses in different applications and
research areas, especially in search engines, which aim
to gain up-to-date data, and where crawlers create a copy
of all pages they visit for later processing. In other words,
search engines index webpages so they can retrieve them
easily and quickly when a user searches for some topic.
Web administrators also use crawlers for automatically
maintaining a website, like examining hyperlinks and
validating HTML tags, or for collecting specific types of
information like email addresses and especially harmful
or spam emails

1. Crawling Space: The crawler starts from a list of
websites of illicit activities. We can depend on a number
of sources like security resources - if available - (i.e.
resources published by governments, or official nongovernmental organizations) [9], or electronic resources
like indexes of Tor network. Links to dark websites can
be attainable even on the surface web or by using generic
search engines (like Google). The crawling space can
expand by adding new links that the crawler finds on the
retrieved webpages, which leads to other pages and so
on.

. Using DOM, CSS and XPath for Data Extraction
To fill data tables, we take advantage of the HTML tags
structure of each page, especially the part represented
by the <body> tag, which includes the requested content.
This content also consists of many other tags with different
types and levels of the HTML structure. Therefore, we
define the Document Object Model (DOM) nodes using
CSS and XPath (response.css and response.xpath)
whichever suitable for the job.
The goal behind this process is to reduce the extracted
elements into rows of corresponding data, in other words
transforming the unstructured data into structured data
initiating it for analysis, it also preserve disk space by
pulling out only the required data from the fetched pages
instead of downloading the whole pages

 Steps to walk through
We have experimented our system on a dark web market
(which we consider not mentioning for security reasons),
and the developed algorithm for that market is as follows:
(Figure (4) illustrates the structure of the market under
study)
1. The crawler starts from the market URL where a login
1
 Scrapy, https://scrapy.org
Figure 4. The dark market structure
interface shows first.
2. Extracts the path of the CAPTCHA image using XPath.
3. Opens the path of the image and saves it to local disk.
4. Processing the downloaded image with pytesseract
library for OCR analyzing.
 58 Journal of Digital Information Management  Volume 17 Number 2  April 2019
5. In case OCR couldn’t be done properly, it gives the
option for manual entry.
6. Sends the credentials of the login form consisting of
the triple: username - password - CAPTCHA value.
7. Calls the procedure of login validation.
8. In case login failed, crawler stops, otherwise, it extracts
Cookies value for the opened session and sends it in the
header of every request.
9. Redirecting the request to the main page and calling
the function of processing the categories of products
listings.
10. For each category, the crawler fetches the category
page and extracts hyperlinks to products pages from the
listing presented in the page, then calling the function of
product processing for each extracted link. After finishing
the current page, the crawler moves forward to the next
page in the category by following its link in the pagination
section at the bottom of the category page, also using
XPath, and so on until the crawler follows all pages in the
category.
11. In the product processing function, the crawler fetches
the product page and extracts the product data using CSS,
the available information about the product includes:
• Product title
• Vendor name
• Origin
• Shipment destination
• Price (in USD)
• Number of sales (since a specific date)
• Number of views (since the same date)
• Date of when mentioned sales and views started
• Moreover, we added the category name to the fields.
In another version of the crawler we also extracted the
vendors’ data, following the same algorithm but calling
the function of vendor processing instead of product, the
available information about the vendor includes:
• Vendor Name
• Vendor Level
• Trust Level
• Positive Feedback (percentage)
• Date of Membership

https://towardsdatascience.com/how-to-scrape-the-dark-web-53145add7033


