dark website. We start from discussing the three layers
also addressed the challenges facing the dark web crawler.
a single dark website under investigation, and can be a

activities are drugs trading, weapons trading, child abuse,
discover in computer systems, or renting a Botnet, which
breach. In addition to trading documents, fake IDs, stolen
financial fraudulence, publicizing criminal ideologies, even
special services that host the security breach activities,
We noticed the difference in definitions of Deep Web and
search engines cannot discover or index, while Dark Web
as the web that consists of all of the aforementioned, in
that part of the hidden and private networks where illicit
how they work:
personal computers distributed on the network around the
internet.
reach.
Freenet, Hyperboria Network, M-Web, Shadow Web and
. What is a Crawler?
following hypertext links and retrieving web documents
research areas, especially in search engines, which aim
search engines index webpages so they can retrieve them
maintaining a website, like examining hyperlinks and
or spam emails
websites of illicit activities. We can depend on a number
like indexes of Tor network. Links to dark websites can
expand by adding new links that the crawler finds on the

structure of each page, especially the part represented
types and levels of the HTML structure. Therefore, we
whichever suitable for the job.
transforming the unstructured data into structured data
instead of downloading the whole pages
We have experimented our system on a dark web market
(Figure (4) illustrates the structure of the market under
1
interface shows first.
4. Processing the downloaded image with pytesseract
5. In case OCR couldn’t be done properly, it gives the
the triple: username - password - CAPTCHA value.
Cookies value for the opened session and sends it in the
the function of processing the categories of products
page and extracts hyperlinks to products pages from the
the current page, the crawler moves forward to the next
XPath, and so on until the crawler follows all pages in the
the product page and extracts the product data using CSS,
• Vendor name
• Price (in USD)
• Date of when mentioned sales and views started
vendors’ data, following the same algorithm but calling
• Vendor Name
• Positive Feedback (percentage)
https://towardsdatascience.com/how-to-scrape-the-dark-web-53145add7033
